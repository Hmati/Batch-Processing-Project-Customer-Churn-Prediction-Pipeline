{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa41a405-72b2-4419-8cb0-0a003b1f7f40",
   "metadata": {},
   "source": [
    "##  02_data_preprocessing\n",
    "\n",
    "The goal of this notebook is to clean and preprocess the data. \n",
    "We'll cover tasks such as handling missing values, encoding categorical variables, and feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ea193-dc8e-40a7-a72f-5bd6d806ead7",
   "metadata": {},
   "source": [
    "#### * Imports and Initialization *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b16b47-6227-4ed0-96db-f7710f7262cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Churn Data Preprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d32cb6-20c1-4658-bd80-ee77f2c84a17",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "we'll load the data from our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6847b6-3117-49a9-ae49-837359803496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from local storage\n",
    "data_path = \"C:/Users/ADMIN/Desktop/Projects/Batch-Processing-Project-Customer-Churn-Prediction-Pipeline/datasets/Bank Customer Churn Prediction.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0785e-6745-4c8b-b610-ebd0b37177df",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "\n",
    "Handling missing data is an essential preprocessing step. We can either drop rows with missing values or fill them with appropriate values (like the mean, median, or mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54808a1a-3c84-4288-832d-759ae4f088dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Option 1: Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Option 2: Fill missing values with median for numerical columns\n",
    "# For categorical columns, you could fill with the mode (most frequent value)\n",
    "df_cleaned['TotalCharges'] = df_cleaned['TotalCharges'].fillna(df_cleaned['TotalCharges'].median())\n",
    "df_cleaned['MonthlyCharges'] = df_cleaned['MonthlyCharges'].fillna(df_cleaned['MonthlyCharges'].median())\n",
    "\n",
    "# Alternatively, for categorical columns like 'PaymentMethod', you can use mode:\n",
    "df_cleaned['PaymentMethod'] = df_cleaned['PaymentMethod'].fillna(df_cleaned['PaymentMethod'].mode()[0])\n",
    "\n",
    "# Verify the result\n",
    "df_cleaned.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157b823-6e8a-4735-9e48-dc3a584c7623",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "Convert categorical columns to numerical values: Use label encoding or one-hot encoding for categorical features like gender, InternetService, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68675d17-04cf-468f-b05a-f78b0238879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numeric using StringIndexer and OneHotEncoder\n",
    "categorical_columns = ['gender', 'InternetService', 'Contract', 'PaymentMethod', 'Partner', 'Dependents']\n",
    "\n",
    "# StringIndexer for label encoding\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + '_Index') for col in categorical_columns]\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoders = [OneHotEncoder(inputCol=col + '_Index', outputCol=col + '_Encoded') for col in categorical_columns]\n",
    "\n",
    "# Combine indexers and encoders in a pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Apply the pipeline to the dataframe\n",
    "df_spark = spark.createDataFrame(df_cleaned)\n",
    "model = pipeline.fit(df_spark)\n",
    "df_transformed = model.transform(df_spark)\n",
    "\n",
    "# Show the transformed data with encoded columns\n",
    "df_transformed.select(categorical_columns + [col + '_Encoded' for col in categorical_columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b3172-436b-48ce-b638-11a31827c5e5",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create new features: For instance, we can create new features such as tenure_months, charges_diff (difference between MonthlyCharges and TotalCharges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af1305-68f7-48e6-9e05-fb3b2bff9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: Example of creating new features\n",
    "\n",
    "df_transformed = df_transformed.withColumn('charges_diff', col('MonthlyCharges') - col('TotalCharges'))\n",
    "\n",
    "# Show the new feature\n",
    "\n",
    "df_transformed.select('charges_diff').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d4a6d-f302-450f-8e02-028ed436f0f9",
   "metadata": {},
   "source": [
    " ### Prepare Final Dataset for Machine Learning\n",
    "Select features and label: Choose the relevant columns for model training. The target variable is Churn, which needs to be encoded (1 for churn, 0 for non-churn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cabec8-4c02-44eb-b6b0-1d8c2e028fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Churn' is the target variable\n",
    "df_transformed = df_transformed.withColumn('Churn', col('Churn').cast('double'))\n",
    "\n",
    "# Select the final features and label columns\n",
    "feature_columns = ['tenure', 'MonthlyCharges', 'TotalCharges', 'charges_diff'] + \\\n",
    "                  [col + '_Encoded' for col in categorical_columns]\n",
    "final_df = df_transformed.select(*feature_columns, 'Churn')\n",
    "\n",
    "# Show the final prepared dataframe\n",
    "final_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92836882-ce48-4181-9cfa-09aabab5b060",
   "metadata": {},
   "source": [
    "###  Save Preprocessed Data to Delta Lake\n",
    "To store the preprocessed data in Delta Lake for future analysis, we'll use Delta format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b255b-0685-4c8f-9c43-0ba6a73ce754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for storing the Delta table in your bucketname that you created \n",
    "delta_path = \"s3a://hmati/preprocessed_data\"\n",
    "\n",
    "# Save the transformed data as Delta table\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "print(\"Preprocessed data saved to Delta Lake.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c08ac-7022-4ee3-8e59-97f06a7f4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Summary and Next Steps\n",
    " summary of the preprocessing steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
