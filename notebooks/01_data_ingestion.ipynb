{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cd9502-6f9f-4a63-8e65-a7b3258927f1",
   "metadata": {},
   "source": [
    "# Load and Validate Raw Data from AWS S3\n",
    "This notebook was to  focus on reading the raw data from S3, inspecting the dataset, and validating its contents but \n",
    "the platform wasnt available at the time . Therefore we use Local File sysytem\n",
    "\n",
    "Recommendation:\n",
    "If scalability is important:\n",
    "\n",
    "Use Google Cloud Storage (GCS) or Azure Data Lake (ADLS) for a seamless cloud alternative to AWS S3. If working locally or on a small project:\n",
    "Use the local file system or MinIO for simple and lightweight storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c424ccf1-5a36-41b7-a1c8-b5e1271e7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\hadoop\\\\bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3e583a-11a3-4f17-bae1-834323bbd442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Sample:\n",
      "+-----------+------------+-------+------+---+------+---------+---------------+-----------+-------------+----------------+-----+\n",
      "|customer_id|credit_score|country|gender|age|tenure|  balance|products_number|credit_card|active_member|estimated_salary|churn|\n",
      "+-----------+------------+-------+------+---+------+---------+---------------+-----------+-------------+----------------+-----+\n",
      "|   15634602|         619| France|Female| 42|     2|      0.0|              1|          1|            1|       101348.88|    1|\n",
      "|   15647311|         608|  Spain|Female| 41|     1| 83807.86|              1|          0|            1|       112542.58|    0|\n",
      "|   15619304|         502| France|Female| 42|     8| 159660.8|              3|          1|            0|       113931.57|    1|\n",
      "|   15701354|         699| France|Female| 39|     1|      0.0|              2|          0|            0|        93826.63|    0|\n",
      "|   15737888|         850|  Spain|Female| 43|     2|125510.82|              1|          1|            1|         79084.1|    0|\n",
      "+-----------+------------+-------+------+---+------+---------+---------------+-----------+-------------+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, when, isnull\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomerChurnDataIngestion\") \\\n",
    "    .config(\"spark.hadoop.fs.file.impl.disable.cache\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load raw customer data from the local file system\n",
    "file_path = r\"C:\\Users\\ADMIN\\Desktop\\Projects\\Batch-Processing-Project-Customer-Churn-Prediction-Pipeline\\datasets\\Bank Customer Churn Prediction.csv\"\n",
    "raw_data = spark.read.csv(f\"file:///{file_path.replace('\\\\', '/')}\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the raw data\n",
    "print(\"Raw Data Sample:\")\n",
    "raw_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2090bc5-ec0b-490c-941d-f753322fce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of Raw Data:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- credit_score: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- balance: double (nullable = true)\n",
      " |-- products_number: integer (nullable = true)\n",
      " |-- credit_card: integer (nullable = true)\n",
      " |-- active_member: integer (nullable = true)\n",
      " |-- estimated_salary: double (nullable = true)\n",
      " |-- churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema of the loaded data\n",
    "\n",
    "print(\"Schema of Raw Data:\")\n",
    "raw_data.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557baf73-82ca-45ae-9cb9-db6c4528d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data Check:\n",
      "+-----------+------------+-------+------+---+------+-------+---------------+-----------+-------------+----------------+-----+\n",
      "|customer_id|credit_score|country|gender|age|tenure|balance|products_number|credit_card|active_member|estimated_salary|churn|\n",
      "+-----------+------------+-------+------+---+------+-------+---------------+-----------+-------------+----------------+-----+\n",
      "|          0|           0|      0|     0|  0|     0|      0|              0|          0|            0|               0|    0|\n",
      "+-----------+------------+-------+------+---+------+-------+---------------+-----------+-------------+----------------+-----+\n",
      "\n",
      "Total Rows: 10000\n",
      "Unique Rows: 10000\n",
      "Number of Duplicate Rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Validate the data (e.g., check for missing values)\n",
    "\n",
    "print(\"Missing Data Check:\")\n",
    "missing_data = raw_data.select([count(when(isnull(c), c)).alias(c) for c in raw_data.columns])\n",
    "missing_data.show()\n",
    "\n",
    "# Check for duplicate rows\n",
    "total_rows = raw_data.count()\n",
    "unique_rows = raw_data.distinct().count()\n",
    "duplicates = total_rows - unique_rows\n",
    "\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Unique Rows: {unique_rows}\")\n",
    "print(f\"Number of Duplicate Rows: {duplicates}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa9ee4-6415-4ba7-b478-27ceec6cf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the validated data for the next step (optional, as Parquet format)\n",
    "#output_path = r\"C:\\Users\\ADMIN\\Desktop\\Projects\\Batch-Processing-Project-Customer-Churn-Prediction-Pipeline\\datasets\\validated_data.parquet\"\n",
    "output_path = \"file:///C:/Users/ADMIN/Desktop/Projects/Batch-Processing-Project-Customer-Churn-Prediction-Pipeline/datasets/validated_data.parquet\"\n",
    "\n",
    "#raw_data.write.parquet(f\"file:///{output_path.replace('\\\\', '/')}\", mode=\"overwrite\")\n",
    "raw_data.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e39450c-3517-49b2-a17d-e6cb3b0ec7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Churn Prediction\") \\\n",
    "    .config(\"spark.hadoop.fs.file.impl.disable.cache\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562aa54e-cc5d-4412-abda-0742dd7c7d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
