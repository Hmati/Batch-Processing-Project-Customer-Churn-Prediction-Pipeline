{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57e15c9-e0a0-475a-835d-1803dad85422",
   "metadata": {},
   "source": [
    "#### Data_storage\n",
    "\n",
    "storing transformed data in Delta Lake, here's a detailed guide.  writing the transformed data to Delta format, which is optimized for efficient storage and fast queries in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dcb70-2ff5-47dd-95f6-031cd6b283ed",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb32571-5905-4e86-bd1e-e75e36ea087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import *\n",
    "\n",
    "# Initialize Spark session with Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Churn Data Storage\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized with Delta Lake.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6c961-955e-4a5e-857a-557e377468fb",
   "metadata": {},
   "source": [
    "### Step 2: Load Transformed Data\n",
    "Here, you will load the transformed data (as a Spark DataFrame) that we created in earlier steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cfe23-6412-4777-b487-466d9d7329b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is already transformed and stored in a DataFrame `df_spark`\n",
    "df_spark.show(5)  # Display the first 5 rows to confirm data\n",
    "\n",
    "# If the data is stored as a CSV file or any other format, load it like this:\n",
    "# df_spark = spark.read.csv(\"path_to_transformed_data.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b70c5a-6eb7-4e01-938e-082639f8dc88",
   "metadata": {},
   "source": [
    "### Step 3: Write Data to Delta Lake\n",
    "Delta Lake is a storage format that provides ACID transactions, scalable metadata handling, and data versioning. Below is the code to write our  Spark DataFrame to Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f265e-5949-402a-ae11-84c09dc1e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Delta table path\n",
    "delta_table_path = \"C:/path_to_store_delta_table/customer_churn_delta\"\n",
    "\n",
    "# Write data to Delta Lake (overwrite if the table already exists)\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Data stored in Delta Lake successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293e2c6-8dc9-4ef1-adaa-261dd5887409",
   "metadata": {},
   "source": [
    "You can replace mode(\"overwrite\") with other modes such as:\n",
    "\n",
    "-append: Adds new data to an existing Delta table.\n",
    "\n",
    "-overwrite: Overwrites the existing Delta table with the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b59dde-8430-43e9-8886-667f2fe70a93",
   "metadata": {},
   "source": [
    "### Step 4: Verify Delta Table Creation\n",
    "To verify that the data has been written correctly to Delta format, we can read it back and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f08704-86e9-44cb-b013-e62872521c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data back from the Delta table\n",
    "df_delta = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Show the data\n",
    "df_delta.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1f133-0792-45b5-86fa-8d3ee3d13a27",
   "metadata": {},
   "source": [
    "### Step 5: Perform Time Travel\n",
    "\n",
    "Delta Lake supports time travel, which allows you to query previous versions of the data. This is useful when you want to roll back to an earlier state of your data or perform audits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036ca53-5706-4f74-957a-2a88d79589bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the history of the Delta table\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table.history().show()  # Show the history of Delta table operations\n",
    "\n",
    "# Query the table as it existed at a specific version\n",
    "df_version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "df_version.show(5)  # Show data from version 0 (or any version you choose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e95c1d-f2bf-4242-b0df-931e3676c54c",
   "metadata": {},
   "source": [
    "### Step 6: Create or Update Delta Table \n",
    "\n",
    "If you need to perform further transformations and update the existing Delta table, you can use Deltaâ€™s MERGE operation for efficient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65b888-3358-40bb-981c-feb899e863c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Example: Merge new data into existing Delta table\n",
    "delta_table.alias(\"old\") \\\n",
    "    .merge(df_spark.alias(\"new\"), \"old.CustomerID = new.CustomerID\") \\\n",
    "    .whenMatchedUpdate(set={\"MonthlyCharges\": \"new.MonthlyCharges\"}) \\\n",
    "    .whenNotMatchedInsert(values={\"CustomerID\": \"new.CustomerID\", \"MonthlyCharges\": \"new.MonthlyCharges\"}) \\\n",
    "    .execute()\n",
    "\n",
    "print(\"Delta table updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322be03-bd6a-47ff-b062-8ff1e44cc36f",
   "metadata": {},
   "source": [
    "### Step 7: Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4981d3-5614-4673-b2e7-3666602f8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data storage in Delta Lake completed successfully.\")\n",
    "print(\"1. Transformed data stored in Delta format.\")\n",
    "print(\"2. Time travel and versioning enabled for future queries.\")\n",
    "print(\"3. Delta table ready for further updates and queries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9086f5c-ec71-4054-975b-7064711968e6",
   "metadata": {},
   "source": [
    "### Summary of Key Points\n",
    "-Delta Lake allows you to store data in a transactional manner and perform time travel.\n",
    "\n",
    "-Use write.format(\"delta\") to store data in Delta format.\n",
    "    \n",
    "-Delta tables support *ACID* transactions, which means you can safely add, delete, and modify data over time.\n",
    "\n",
    "-You can perform time travel and query historical versions of the data using the versionAsOf option.\n",
    "    \n",
    "-The MERGE operation can be used for efficient upserts into Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d48a85-c3c5-4584-9f06-b7231b1d74d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
