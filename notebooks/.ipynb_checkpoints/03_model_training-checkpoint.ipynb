{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74d6bd2-47e3-4b42-a81c-de8316d776a8",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "\n",
    "Let's proceed with creating a notebook for training a machine learning model to predict customer churn. \n",
    "We will focus on both Logistic Regression and Random Forest classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9eedc-682b-43fc-980f-8aebc24216de",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb53e13-6f0b-4691-9b98-08e9b0afb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Churn Model Training\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05331901-faca-48fb-b6a7-96c0a7cf3961",
   "metadata": {},
   "source": [
    "### Step 2: Load the Preprocessed Data\n",
    "\n",
    "The data is stored in a Delta Lake format or CSV, load it to continue from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2c162-0fff-4475-842e-d7e7e0708a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data (from Delta or CSV)\n",
    "# If using Delta Lake:\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"s3a://your-bucket-name/preprocessed_data\")\n",
    "\n",
    "# If using CSV (ensure the file path is correct)\n",
    "data_path = \"C:/Users/ADMIN/Desktop/Projects/Batch-Processing-Project-Customer-Churn-Prediction-Pipeline/datasets/processed_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert it to a Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "# Check the schema and first few rows\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b386833-67ed-4e25-9d0e-08559603fd64",
   "metadata": {},
   "source": [
    "### Step 3: Feature Selection and Vectorization\n",
    "Assemble features into a single vector: Machine learning models in Spark require the features to be in a vector format. Weâ€™ll use the VectorAssembler to combine all feature columns into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77de44-db39-47e3-ab4e-d90ca810d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of feature columns\n",
    "feature_columns = ['tenure', 'MonthlyCharges', 'TotalCharges', 'charges_diff'] + \\\n",
    "                  [col + '_Encoded' for col in categorical_columns]\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_spark = assembler.transform(df_spark)\n",
    "\n",
    "# Show the assembled features\n",
    "df_spark.select('features').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed4874-29b3-471d-917c-39ba37fc8ade",
   "metadata": {},
   "source": [
    "### Step 4: Train-Test Split\n",
    "\n",
    "Split the dataset into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefaef3-cff8-4b0b-bfea-caa2bc873763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (80%) and testing (20%)\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Show the sizes of the train and test datasets\n",
    "print(f\"Training Data Size: {train_data.count()}\")\n",
    "print(f\"Test Data Size: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa24b7a-e6fd-4eec-851d-82b29c2de4eb",
   "metadata": {},
   "source": [
    "### Step 5: Train Logistic Regression Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e35d3f-6ac4-4eb8-99b4-0d62e5941da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Churn\")\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show the predictions\n",
    "lr_predictions.select(\"Churn\", \"prediction\", \"probability\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cfcf0-d363-41f1-aeb6-99d13ebae8ea",
   "metadata": {},
   "source": [
    "### Step 6: Train Random Forest Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949882d5-ca14-4229-b225-e1cec22029b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Churn\")\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Show the predictions\n",
    "rf_predictions.select(\"Churn\", \"prediction\", \"probability\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20fcd48-4446-44da-b5c2-3f03f51d07db",
   "metadata": {},
   "source": [
    "### Step 7: Model Evaluation\n",
    "Evaluate both models using a Binary Classification Evaluator to compute metrics like Accuracy, ROC AUC, and F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4e770-3d87-48aa-98ad-febfa3fb36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\", rawPredictionCol=\"prediction\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_auc = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression AUC: {lr_auc}\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest AUC: {rf_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40363726-97ec-4cf3-9776-71b5b8c2314f",
   "metadata": {},
   "source": [
    "### Step 8: Hyperparameter Tuning \n",
    "\n",
    "You can perform hyperparameter tuning using Grid Search and Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd0c84-0657-431e-8440-46f3a864d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.01])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
    "             .build())\n",
    "\n",
    "# Initialize CrossValidator\n",
    "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross-validation to find the best model\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Get the best model\n",
    "best_lr_model = cv_model.bestModel\n",
    "print(\"Best Logistic Regression Model:\", best_lr_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbe3c9-ee59-4612-9244-ff95c3627d2f",
   "metadata": {},
   "source": [
    "### Step 9: Model Export \n",
    "Once the model is trained, you can save the model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb116af-3f9a-452a-b58a-e19e5cb73ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Logistic Regression model\n",
    "lr_model.save(\"path_to_save_lr_model\")\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "rf_model.save(\"path_to_save_rf_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72effc-2ac8-4b9b-898a-e90040f82ac8",
   "metadata": {},
   "source": [
    "### Step 10: Conclusion\n",
    "Summarize the model performance and provide the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d3782-755f-419e-b5e2-53176b5dde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model training completed.\")\n",
    "print(\"1. Logistic Regression and Random Forest models trained.\")\n",
    "print(\"2. Hyperparameter tuning performed (if applicable).\")\n",
    "print(\"3. Models evaluated based on AUC.\")\n",
    "print(\"4. Models saved for deployment.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
