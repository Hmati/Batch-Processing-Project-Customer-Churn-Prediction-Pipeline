{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cd9502-6f9f-4a63-8e65-a7b3258927f1",
   "metadata": {},
   "source": [
    "# Load and Validate Raw Data \n",
    "This notebook was to  focus on reading the raw data from S3, inspecting the dataset, and validating its contents but \n",
    "the platform wasnt available at the time . Therefore we use Local File sysytem\n",
    "\n",
    "Recommendation:\n",
    "If scalability is important:\n",
    "\n",
    "Use Google Cloud Storage (GCS) or Azure Data Lake (ADLS) for a seamless cloud alternative to AWS S3. If working locally or on a small project:\n",
    "Use the local file system or MinIO for simple and lightweight storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c3271-fd14-4778-96d7-406dd69bfb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Churn Data Ingestion\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized.\")\n",
    "\n",
    "# Define dataset path (local or S3)\n",
    "# Change the path to your dataset file or S3 bucket if using AWS\n",
    "dataset_path = r\"C:\\Users\\ADMIN\\Desktop\\Projects\\Batch-Processing-Project-Customer-Churn-Prediction-Pipeline\\datasets\\Bank Customer Churn Prediction.csv\"\n",
    "\n",
    "# Load the dataset into Spark DataFrame\n",
    "try:\n",
    "    churn_data = spark.read.csv(dataset_path, header=True, inferSchema=True)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "churn_data.show(5)\n",
    "\n",
    "# Print the schema to understand the data structure\n",
    "print(\"Dataset Schema:\")\n",
    "churn_data.printSchema()\n",
    "\n",
    "# Count the number of rows and columns\n",
    "row_count = churn_data.count()\n",
    "column_count = len(churn_data.columns)\n",
    "print(f\"The dataset contains {row_count} rows and {column_count} columns.\")\n",
    "\n",
    "# Basic Data Exploration\n",
    "print(\"Summary Statistics:\")\n",
    "churn_data.describe().show()\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "missing_values = churn_data.select(\n",
    "    *[col(c).isNull().alias(c) for c in churn_data.columns]\n",
    ").groupBy().sum()\n",
    "\n",
    "print(\"Missing Values per Column:\")\n",
    "missing_values.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc235f49-13b2-4936-989b-dd1dce8a26c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
